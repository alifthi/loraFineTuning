MODEL_NAME='Qwen/Qwen1.5-0.5B-Chat'
DATA_DIR='../data/alpaca'

'''LoRA configs'''
LORA_R=64
LORA_ALPHA=16
LORA_DROPOUT=0.1
TASK_TYPE='CAUSAL_LM'

'''Training Parametrs'''
OUTPUTS_DIR='../outputs/'
EPOCHS=32
LEARNING_RATE=0.0005
WEIGHT_DECAY=0.001
OPTIMIZER='paged_adamw_32bit'
LR_SCHEDULAR='cosine'
SAVE_STEPS=0
MAX_SEQ_LENGTH=128